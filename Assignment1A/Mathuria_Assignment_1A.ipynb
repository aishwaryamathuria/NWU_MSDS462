{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eI_SydE4vshM"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "  <h3><center>MSDS-462: Computer Vision</center></h3>\n",
    "  <h2><center>Assignment 1A: Lab 1 - An Image is Just Numbers</center></h2>\n",
    "  <b>Author</b>: Aishwarya Mathuria\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKkE__EYvshN"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2>Setup</h2>\n",
    "    <p>Import torch, torchvision, and cv2 (OpenCV).</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jB5pWpU3vshN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RnWwlqJOg0U"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2>Load Image</h2>\n",
    "    <p>Use cv2.imread() to load a sample image from a URL or file path.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aOUQ1RAavshN"
   },
   "outputs": [],
   "source": [
    "image_url = \"https://raw.githubusercontent.com/aishwaryamathuria/NWU_MSDS462/refs/heads/main/Assignment1A/test_image.jpg\"\n",
    "response = requests.get(image_url)\n",
    "image_path = \"temp_image.jpg\"\n",
    "with open(image_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "image_bgr = cv2.imread(image_path)\n",
    "\n",
    "# Convert BGR to RGB\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssjntHjEvshN"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2>Convert to Tensor</h2>\n",
    "    <p>Use torchvision.transforms.ToTensor() to convert the image into a PyTorch tensor.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GcgBaEGQvshO"
   },
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "image_tensor = to_tensor(image_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VP__s4_Og0V"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2>Inspect the Tensor</h2>\n",
    "    <ul>\n",
    "        <li>Print the tensor to observe that it's just a collection of numbers.</li>\n",
    "        <li>Print the tensor's .shape, .dtype, and .device.</li>\n",
    "        <li>In a markdown cell, explain what each dimension of the shape represents (e.g., for an RGB image, what do the three dimensions stand for?).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zywaNvxQOg0W",
    "outputId": "7672f5d9-c7b6-42ad-e26e-8242f2e5646d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor values:\n",
      "tensor([[[0.6863, 0.6824, 0.6667,  ..., 0.5020, 0.5020, 0.5020],\n",
      "         [0.6902, 0.6824, 0.6706,  ..., 0.5059, 0.5059, 0.5098],\n",
      "         [0.6941, 0.6863, 0.6667,  ..., 0.5137, 0.5176, 0.5176],\n",
      "         ...,\n",
      "         [0.0314, 0.0941, 0.0471,  ..., 0.2706, 0.1765, 0.1216],\n",
      "         [0.0196, 0.1098, 0.0235,  ..., 0.3765, 0.2784, 0.2784],\n",
      "         [0.0078, 0.0784, 0.0196,  ..., 0.3098, 0.2667, 0.3333]],\n",
      "\n",
      "        [[0.4824, 0.4784, 0.4588,  ..., 0.3333, 0.3333, 0.3333],\n",
      "         [0.4863, 0.4784, 0.4627,  ..., 0.3373, 0.3373, 0.3412],\n",
      "         [0.4902, 0.4824, 0.4706,  ..., 0.3412, 0.3490, 0.3490],\n",
      "         ...,\n",
      "         [0.0275, 0.0980, 0.0510,  ..., 0.2824, 0.1843, 0.1294],\n",
      "         [0.0235, 0.1137, 0.0275,  ..., 0.3882, 0.2863, 0.2863],\n",
      "         [0.0157, 0.0863, 0.0275,  ..., 0.3333, 0.2902, 0.3569]],\n",
      "\n",
      "        [[0.5725, 0.5686, 0.5608,  ..., 0.4510, 0.4431, 0.4431],\n",
      "         [0.5765, 0.5686, 0.5647,  ..., 0.4549, 0.4471, 0.4510],\n",
      "         [0.5804, 0.5725, 0.5686,  ..., 0.4706, 0.4667, 0.4667],\n",
      "         ...,\n",
      "         [0.0078, 0.0745, 0.0275,  ..., 0.0588, 0.0000, 0.0000],\n",
      "         [0.0039, 0.0941, 0.0039,  ..., 0.1725, 0.1020, 0.1020],\n",
      "         [0.0118, 0.0824, 0.0157,  ..., 0.0431, 0.0078, 0.0745]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensor values:\")\n",
    "print(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kggpWhn5Og0W",
    "outputId": "e6625781-33c5-43f7-b990-5c996277d599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor shape: torch.Size([3, 183, 275])\n",
      "Tensor dtype: torch.float32\n",
      "Tensor device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTensor shape:\", image_tensor.shape)\n",
    "print(\"Tensor dtype:\", image_tensor.dtype)\n",
    "print(\"Tensor device:\", image_tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfo3e6dJOg0W"
   },
   "source": [
    "<div style=\"background: #e6e6fa; padding: 15px 30px; border: 1px solid #a3a3eb\">\n",
    "    <h2>Explanation</h2>\n",
    "    <p>The tensor shape here is [3, 183, 275] which maps to (C, H, W) which is a standard format expected by PyTorch.</p>\n",
    "    <p>The first dimension C here has value (3) which represents the number of color channels in the image. Since this is an RGB image, there are three channels: <strong>Red, Green, and Blue</strong>.</p>\n",
    "    <p>The second dimension H here has value (183) which represents the height of the image which is 183px measured along the vertical axis.</p>\n",
    "    <p>The third dimension W here has value (275) which represents the width of the image which is 275px measured along the horizontal axis.</p>\n",
    "    <p>The tensorâ€™s data type is float32, and all pixel values are normalized to the range 0.0 to 1.0. The ToTensor() here has normalized the original uint8 pixel values (which range from 0 to 255) into floating-point values suitable for neural networks. Each number in the tensor corresponds to the intensity of a specific color channel at a particular pixel location.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4QOuDnqOg0W"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2>Simple Manipulation with OpenCV</h2>\n",
    "    <p>Use a simple OpenCV function like cv2.cvtColor() to convert the image to grayscale and repeat the inspection of the tensor's shape.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HWdJP1RSOg0W"
   },
   "outputs": [],
   "source": [
    "gray_image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)\n",
    "gray_tensor = to_tensor(gray_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D5-dPkjTjkx",
    "outputId": "15cb3b1a-9569-478b-aab7-ef5a3d189b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor values:\n",
      "tensor([[[0.5529, 0.5490, 0.5333,  ..., 0.3961, 0.3961, 0.3961],\n",
      "         [0.5569, 0.5490, 0.5373,  ..., 0.4000, 0.4000, 0.4039],\n",
      "         [0.5608, 0.5529, 0.5412,  ..., 0.4078, 0.4118, 0.4118],\n",
      "         ...,\n",
      "         [0.0275, 0.0941, 0.0471,  ..., 0.2549, 0.1608, 0.1137],\n",
      "         [0.0196, 0.1098, 0.0235,  ..., 0.3608, 0.2627, 0.2627],\n",
      "         [0.0118, 0.0824, 0.0235,  ..., 0.2941, 0.2510, 0.3176]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensor values:\")\n",
    "print(gray_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDCLXn_aTmNh",
    "outputId": "18da1c3b-bbfc-45ea-d9c3-9621203cbdad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor shape: torch.Size([1, 183, 275])\n",
      "Tensor dtype: torch.float32\n",
      "Tensor device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTensor shape:\", gray_tensor.shape)\n",
    "print(\"Tensor dtype:\", gray_tensor.dtype)\n",
    "print(\"Tensor device:\", gray_tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNiuJOYybb7O"
   },
   "source": [
    "<div style=\"background: #e6e6fa; padding: 15px 30px; border: 1px solid #a3a3eb\">\n",
    "    <h2>Explanation</h2>\n",
    "    <p>After converting the image to grayscale, the tensor shape (C, H, W) changed from [3, 183, 275] to [1, 183, 275]. The first dimension here is now (1) which shows that in a grayscale image there is only a single channel because each pixel is described by one intensity value rather than separate red, green, and blue components. This single value represents the brightness of the pixel.</p>\n",
    "    <p>The second dimension (183) represents the height of the image which is 183 px and this remains unchanged from the original RGB image because grayscaling does not affect image resolution. The third dimension (275) represents the width of the image which is 275px and remains unchanged after conversion to grayscale as well. The tensor's data type is float32, and the pixel values are normalized to the range 0.0 to 1.0, just like in the RGB case. These values represent grayscale intensity, where lower values correspond to darker pixels and higher values correspond to brighter pixels.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QF905S07Og0W"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2>Reflect</h2>\n",
    "    <p>In a final markdown cell, write 2-3 sentences answering the prompt: \"After seeing an image represented as a tensor, what do you think is the biggest challenge for a computer trying to identify the main object in that image?\"</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SK_M4RiOg0W"
   },
   "source": [
    "<div style=\"background: #e6e6fa; padding: 15px 30px; border: 1px solid #a3a3eb\">\n",
    "    <h2>Reflection</h2>\n",
    "    <p>In my opinion, the biggest challenge for a computer trying to identify an image is that the computer only sees a grid of numbers, not an object the way a human does. It has to learn which parts of those numbers represent the main object and which parts are just background and hence small variations in how an object looks, backgrounds, shadows and lighting changes can easily confuse the model, even when the main object feels obvious to a human.</p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
